#### Table 1: General
| Paper                                                                                                                                                                                          | Year | Description | Model Name | Model Size | Is the Code Available? | GitHub link | Evaluation Methods | Training Data |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- | ----------- | ---------- | ---------- | ---------------------- | ----------- | ------------------ | ------------- |
| [Mamba: Linear-time sequence modeling with selective state spaces](https://arxiv.org/abs/2312.00752)                                                                                           | 2023 | Mamba is a new SSM (State Space Models) based architecture that may dethrone transformers. | Mamba      | 130M - 2.8B (for pretrained models) | Yes                    | https://github.com/state-spaces/mamba | Zero-shot evaluation using [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/big-refactor). ***Mamba is best-in-class on every single evaluation result, and generally matches baselines at twice the model size.*** | Pretrained language models have been trained on the [pile](https://huggingface.co/datasets/EleutherAI/pile) dataset. |
| [Combining recurrent, convolutional, and continuous-time models with linear state space layers](https://arxiv.org/abs/2110.13985) | 2021 | TODO           | TODO          | TODO          | TODO                      | TODO           | TODO                  | TODO             |
| [Efficiently modeling long sequences with structured state spaces](https://arxiv.org/abs/2111.00396)                                                                                           | 2021 | The Structured State Space for Sequence Modeling (S4) architecture is a new approach to very long-range sequence modeling tasks for vision, language, and audio, showing a capacity to capture dependencies over tens of thousands of steps. Especially impressive are the modelâ€™s results on the challenging Long Range Arena benchmark, showing an ability to reason over sequences of up to 16,000+ elements with high accuracy.           | S4          | 249M          | Yes                      | https://github.com/state-spaces/s4?tab=readme-ov-file           | Evaluated on [Long-Range Arena (LRA)](https://github.com/google-research/long-range-arena) benchmark as well as on other datasets in various tasks including image and speech classification, image and text generation, and time series forecasting.
| [SC09](https://huggingface.co/datasets/krandiash/sc09), [WikiText-103](https://paperswithcode.com/dataset/wikitext-103)m [Permuted MNIST](https://paperswithcode.com/dataset/permuted-mnist) and [Others](https://github.com/state-spaces/s4/tree/main/configs/pipeline) |
| [Simplified state space layers for sequence modeling](https://arxiv.org/abs/2208.04933)                                                                                                        | 2022 |  This paper introduces stacking many dense, multi-input, multi-output (MIMO) state space models (SSMs) as a layer. This replaces the many single-input, single-output (SISO) SSMs used by the structured state space sequence (S4) model. This allows making use of efficient parallel scan to achieve the same computational effiency of S4, without the need to use frequency domain and convolutional methods. S5 achieves the same, if not better, performance than S4 on a range of long-range sequence modeling tasks.           | S5          | TODO          | Yes                      | https://github.com/lindermanlab/S5           | Evaluated on [Long-Range Arena (LRA)](https://github.com/google-research/long-range-arena) benchmark as well as on other datasets in various tasks including speech and Pixel-Level 1-D image classification | [Long-Range Arena (LRA)](https://github.com/google-research/long-range-arena), [sMNIST](https://github.com/dsalaj/sMNIST), psMNIST and others         |

#### Table 2: Music Information Retrieval related
| Paper                                                                                                                                                                                          | Year | Description | Model Name | Model Size | Is the Code Available? | GitHub link | Evaluation Methods | Training Data |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- | ----------- | ---------- | ---------- | ---------------------- | ----------- | ------------------ | ------------- |
| [Evaluating generative audio systems and their metrics](https://arxiv.org/abs/2209.00130)                                                                                                      | 2022 | This paper presents a study that investigates state-of-the-art approaches side-by-side with a set of previously proposed objective metrics for audio reconstruction, and with a listening study          | -          | -          | -                      | -           | -                  | [Nsynth](https://magenta.tensorflow.org/datasets/nsynth)             |
| [On the evaluation of generative models in music](https://www.researchgate.net/publication/328728367_On_the_evaluation_of_generative_models_in_music)                                                                                | 2018 | This paper proposes a set of simple musically informed objective metrics enabling an objective and reproducible way of evaluating and comparing the output of music generative systems           | -          | -          | -                      | -           | -                  | -             |
| [MidiTok: A python package for MIDI file tokenization](https://arxiv.org/abs/2310.17202) | 2023 | MidiTok is a Python package that provides utilities for tokenizing MIDI files into sequences of tokens, and detokenizing them back into MIDI files. It supports various tokenization strategies. | MidiTok | - | Yes | [https://github.com/Natooz/MidiTok](https://github.com/Natooz/MidiTok) | - | - |
| [MuseCoco: Generating Symbolic Music from Text](https://arxiv.org/abs/2306.00110)                                                                                                              | 2023 | TODO           | MuseCoco   | TODO          | TODO                      | TODO           | TODO                  | TODO             |
