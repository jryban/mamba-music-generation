defaults:
  - model: mamba
  - _self_

data:
  path: data
  dataset_name: maestro
  tokenizer:
    type: tsd  # remi | midiike | tsd, structured | cpword | octuple | mumidi | mmm
    training_model: BPE  # BPE | Unigram | WordPiece
  train_split: 0.8
  max_seq_len: 1024
  test_train_on_one_file: True
  test_train_on_one_file_path: ""

training:
  num_workers: 15 # lower this setting in case of workers segmentation fault error
  batch_size: 3
  epochs: 60
  learning_rate: 1e-4
  step_size: 3
  gamma: 0.1
  callbacks:
    patience: 10


wandb:
  project: "WIMU mamba-music-generation"
  entity: "wut-zzsn"
  cleanup:
    dry_run: False
    run_cleanup_every_epochs: 5

models:
  save_path: models

inference:
  wandb_model_full_name: "wut-zzsn/WIMU mamba-music-generation/model-wnz2pevd:v2" # example: "wut-zzsn/WIMU mamba-music-generation/model-mn6cowxw:v4"
  model_path: "models/model-mn6cowxw:v4.ckpt" # optional, if you want to load model that is not in wandb fill this and leave wandb_model_full_name empty
  input_length: 1   # Example length
  input_ids: [1]      # list of input ids (optional, if not provided, random input will be generated)
  max_length: 1024
  temperature: 0.8
  top_k: 50
  repetition_penalty: 0.5